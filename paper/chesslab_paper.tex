\documentclass[10pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{natbib}
\usepackage[margin=0.75in]{geometry}
\usepackage{abstract}
\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{url}

% Set up abstract formatting
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\small}

% Keywords command
\providecommand{\keywords}[1]
{
  \small
  \textbf{\textit{Keywords---}} #1
}

\lstset{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  commentstyle=\itshape\color{gray},
  keywordstyle=\bfseries\color{blue},
  numberstyle=\tiny\color{gray},
  numbersep=5pt,
  stringstyle=\color{orange},
  frame=single,
  framesep=3pt,
  framerule=0.5pt,
  rulecolor=\color{gray!30},
  backgroundcolor=\color{gray!5}
}

\title{ChessLab: Evaluating the Wisdom of Artificial Crowd in Chess}
\author{Mario Larsen}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  \emph{Wisdom of crowds} is a phenomenon stating that aggregated decisions of diverse, independent agents often outperform individual experts.
  However, crowds of casual chess players still struggle against strong opponents.
  Recent projects tend to show that the method by which a move is chosen by the crowd can significantly influence their performance.
  In this work, we introduce \textbf{ChessLab}, an open-source framework for studying artificial collective intelligence in sequential decision-making through chess.
  By replacing human crowds with heterogeneous ensembles of deterministic and learning-based chess engines, we began investigating questions such as:
  (1) Can voting ensembles of weak agents defeat stronger opponents?
  (2) How does diversity composition affect collective performance?
  (3) What voting mechanisms optimally aggregate engine decisions?
  This paper presents the framework, outlines our experimental methodology, and provides a comprehensive research roadmap for studies to be conducted during the next phase of this project.

  \keywords{chess engines, wisdom of crowds}
\end{abstract}

\section{Introduction}

\emph{Collective intelligence} can in some circumstances exceed individual capability.
One of the earliest demonstrations comes from Galton's famous ox-weighing experiment, where the median guess of approximately 787 individuals in a county fair was more accurate than expert estimators and nearly matched the actual weight of 1,197 pounds~\cite{galton1907}.

This phenomenon has been tested in a variety of settings, such as prediction markets, where Atanasov et al.\ compared prediction markets with prediction polls in a large geopolitical forecasting tournament and found that carefully aggregated team forecasts could outperform market prices~\cite{atanasov2017}.
Question-answering game shows also provide natural experiments.
In ``Who Wants to Be a Millionaire'', the ``Ask the Audience'' lifeline yields correct answers around 87-92\% of the time, whereas phone-a-friend experts succeed only about 55-65\% of the time, across many national editions of the show~\cite{surowiecki2004}.
However, in competitive games like chess, crowds have historically struggled against strong opponents.

The 1999 Kasparov vs.\ The World match saw more than 50,000 human players collectively challenge the World Chess Champion Garry Kasparov~\cite{kasparov1999}, reaching move 62 before conceding.
More recently, in May 2025, over 143,000 players held World Champion Magnus Carlsen to a draw in a record-breaking game on Chess.com~\cite{chess2025}, and in November 2025, over 200,000 players loose in a match against International Master Levy Rozman (GothamChess)~\cite{gothamchess2025}.
However, these matches remain largely anecdotal: a single game provides minimal evidence for systematic collective intelligence, as outcome variance is high and learning effects are confounded.

A Fouloscopie project, led by Mehdi Moussaïd at the Max Planck Institute for Human Development, addressed this limitation by conducting the first rigorous, large-scale experiment on collective chess intelligence~\cite{moussaid2025}.
Approximately 25,000 participants were split into teams to play a total of 500 chess games against AI opponents.
Using majority-vote aggregation of player moves, Moussaïd demonstrated that crowds achieved a performance level similar to a master while being composed predominantly of beginners and casual players.
Importantly, by controlling for poll visibility, the results showed a significant effect of social influence on the crowd's level.

Yet even this landmark study leaves open critical questions:
\begin{itemize}
  \item \textbf{Mechanism understanding}: What aspects of diversity (in skill, size, architecture) drive the ensemble advantage?
  \item \textbf{Optimization}: Can we identify decision-making processes that outperform simple majority aggregation?
  \item \textbf{Domain generality}: Does collective intelligence in chess rely on human-specific cognitive mechanisms, or do the principles apply to artificial agents?
\end{itemize}

We address these questions through \textbf{ChessLab}, an open-source framework for studying artificial collective intelligence in chess.
By substituting human crowds with heterogeneous ensembles of chess engines, we gain precise control over experimental conditions, perfect reproducibility, and the ability to run experiments rapidly with minimal computational cost.

The main contributions of this paper are:

\begin{enumerate}
  \item \textbf{Framework and methodology}: A modular, extensible system for running ensemble chess experiments at scale, with persistent storage of games, moves, evaluations, and statistical analyses.
  \item \textbf{Theoretical grounding}: Clear hypotheses derived from Scott Page's Diversity Prediction Theorem and classical wisdom-of-crowds theory, adapted to the chess domain.
  \item \textbf{Reproducible baselines}: Quantitative comparison against Fouloscopie human results, enabling assessment of whether artificial and human collective intelligence follow similar principles.
  \item \textbf{Comprehensive research roadmap}: A structured plan for experiments addressing collective intelligence across different engine types, ensemble compositions, and voting strategies.
\end{enumerate}

This paper focuses on framework description, experimental design, and hypothesis articulation.
Results from additional experiments will be presented in subsequent publications as they are completed.

\section{Related Work}

\subsection{Collective Intelligence and Wisdom of Crowds}

The wisdom of crowds phenomenon emerged from early empirical observations:
Galton's ox-weighing experiment demonstrated that collective judgments (median of 787 guesses) were more accurate than nearly all individual estimates~\cite{galton1907}.
Surowiecki systematized this observation, arguing that under certain conditions (diversity of opinion, independence of members, decentralization, and effective aggregation) groups reliably outperform experts~\cite{surowiecki2004}.

Formally, Scott Page's \emph{Diversity Prediction Theorem} provides mathematical grounding:
\begin{equation}
  \text{Collective Error} = \overline{\text{Individual Error}} - \text{Diversity}
\end{equation}
where collective error is the mean squared error of the group prediction, \(\overline{\text{Individual Error}}\) is the average of individual squared errors, and Diversity captures how different agents' predictions are~\cite{page2007}.
This result implies that a team of common individuals can outperform a specialist, provided the group is large enough and exhibits sufficient diversity in cognitive approach.

However, diversity alone is insufficient.
Studies show that social influence, information cascades, and herding can destroy the diversity necessary for wisdom of crowds.
Almaatouq et al.\ demonstrated that agents strategically choosing whom to learn from (adaptive networks) better maintain collective intelligence than static networks~\cite{almaatouq2020}.
Koriat and colleagues showed that \emph{confidence} acts as a metacognitive signal:
when individuals can estimate how well-calibrated their judgment is, confidence-weighted aggregation outperforms simple majority voting~\cite{koriat2011}, except when facing common misconceptions that result in misplaced confidence.
Prelec extended this with the ``surprisingly popular'' voting method, which selects answers that are more popular than subjects predicted, revealing hidden consensus even when the majority is wrong~\cite{prelec2017}.

In summary, collective intelligence requires:
(1) a sufficient number of participants,
(2) diversity in decision-making,
(3) independence of judgments,
(4) appropriate aggregation mechanisms, and possibly
(5) calibration of confidence when weighting is available.

\subsection{Collective Intelligence in Chess}

Chess provides an ideal laboratory for studying collective decision-making in complex, sequential tasks.
Unlike static judgment tasks (trivia answering, estimation), chess involves:
\begin{itemize}
  \item \textbf{High-dimensional state space}: Approximately \(10^{47}\) legal positions, making expert heuristics difficult and providing an extensive test dataset.
  \item \textbf{Strategic thinking}: Each player must evaluate not just the next move but plan for responses and counter-responses. This task allows deliberation to express emerging strategies.
  \item \textbf{Objective evaluation}: The outcome of a game is unambiguous, and thanks to modern engines, each move can be evaluated with precise numerical scoring. The performance level of a crowd can be estimated with arbitrary precision.
  \item \textbf{Cultural significance}: Chess remains a synecdoche for intelligence. Both humans and algorithms are expected to demonstrate competence at this task, providing abundant documentation and research literacy.
\end{itemize}


To evaluate the level of a chess player the community mainly rely on a number called the Elo.
Given two players with Elo ratings \(\text{Elo}_A\) and \(\text{Elo}_B\), the expected score (probability of A winning) is:
\begin{equation}
  \label{eqn:expected}
  E_A = \frac{1}{1 + 10^{(\text{Elo}_B - \text{Elo}_A) / 400}}
\end{equation}
This is the standard Elo formula, calibrated for chess~\cite{elo1978}.

While this metric captures relative strength between two players, its absolute interpretation depends heavily on methodology and context.
Community conventions suggest that players understanding the basic rules rate between 600 and 900 Elo, casual players typically orbit around 1300 Elo, and from 2000 Elo onward, players enter the master and grandmaster territory.
At his peak in May 2014, Magnus Carlsen achieved a classical rating of 2882, the highest in history.
He also achieved 2909 in the newly-introduced Freestyle Chess (Chess960) rating system~\cite{carlsen2025}.

\subsection{Empirical Studies of Crowd Chess}

The Kasparov vs.\ The World match (1999) is the earliest documented crowdsourced chess challenge.
Over four months, approximately 50,000 amateur and intermediate players voted on moves against the then-World Champion.
Despite reaching an advanced position, the crowd eventually conceded after 62 moves when facing maximum resistance from Kasparov~\cite{kasparov1999}.
While historically significant, this single-game format provides weak evidence: a single outcome cannot be reliably attributed to collective intelligence versus chance.

The Fouloscopie experiment rectified this limitation through large-scale repetition~\cite{moussaid2025}.
Moussaïd and colleagues organized approximately 25,000 human participants to play a total of 500 games against chess engine opponents.
Key design features included:
\begin{itemize}
  \item \textbf{Diverse player population}: Participants self-reported their Elo rating; the population was approximately normally distributed with a mean around 1165 Elo.
  \item \textbf{Majority-vote aggregation}: Players had several hours to evaluate positions and vote on the next move; the most-voted move was played automatically.
  \item \textbf{Opponent variety}: Chess engine opponents (Maia models) ranged from weak (1100 Elo) to strong (1900 Elo), allowing measurement of performance across the strength spectrum.
  \item \textbf{Social influence manipulation}: Comparing voting-only versus poll-visibility conditions revealed how information visibility affects collective decision quality.
\end{itemize}

Results showed that crowds achieved a win rate exceeding 60\% across all tested opponents.
With poll visibility, win rates improved to approximately 77\%, while visibility-hidden conditions showed approximately 64\% win rates.
Importantly, the crowd played measurably better than the average individual member, confirming classic wisdom-of-crowds predictions.

\subsection{Chess Engines}

Modern chess engines span multiple paradigms, each with distinct decision-making processes.
We focus on three particularly relevant approaches:

\subsubsection{Stockfish}

Stockfish, as of 2025, is the world's strongest chess engine.
It is an open-source tree-search algorithm combining classical techniques (alpha-beta pruning, endgame tables) with small neural networks for position evaluation~\cite{stockfish2024}.
Its strength is calibrated via Elo ratings: users can set \texttt{UCI\_Elo} to any value from 1320 to 3190.
The engine deliberately weakens its play through stochastic evaluation perturbation (seeded random noise), ensuring exploration of different move trees at reduced strength levels.

\subsubsection{Maia: Human-Mimetic Neural Networks}

In contrast to optimal play, Maia learns to \emph{predict human moves} from millions of Lichess games~\cite{mcilroy-young2020}.
Rather than computing optimal moves, Maia generates a probability distribution over legal moves, parameterized by player skill level.
Maia-2, introduced at NeurIPS 2024, incorporates a skill-aware attention mechanism that dynamically integrates player skill level with board position encoding~\cite{maia2024}.
This mechanism achieves approximately 50--52\% top-1 move accuracy across human players rated 1100 to 1900 Elo.
In practical play, Maia replicates human decision-making nearly indistinguishably, making it an excellent tool for evaluating crowd performance against human-like opponents.

\subsubsection{Large Language Models}

Recent work demonstrates that Large Language Models (LLMs) trained on chess game transcriptions can develop emergent internal representations of board states~\cite{karvonen2024}.
These representations can be altered, thereby changing the move predicted by the LLM.
This represents a fundamentally different paradigm: LLMs make decisions based on statistical pattern recognition from training data rather than explicit search or neural evaluation.
Their move quality is unstable and depends on a myriad of factors (model architecture, PGN-based prompting, board representation, move history), which are often difficult to replicate or interpret.

Carlini showed that \texttt{gpt-3.5-turbo-instruct} can achieve a playing strength of roughly 1,788 Elo under specific conditions~\cite{carlini2023}.
More recent transformer-based models have achieved grandmaster-level performance when trained directly on game transcriptions~\cite{schrittwieser2024}.

Although modern chat-oriented LLMs appear to lag behind, Dynomight~\cite{dynomight2024} demonstrated that prompting strategies—such as requiring the model to regenerate the move history before outputting the next move—can dramatically improve their move quality.
These results suggest that apparent “unusual” chess behavior in some LLMs may stem less from intrinsic chess competence and more from specific training setups and prompt configurations.

For ensemble experiments, LLMs offer the greatest cognitive diversity and potential for open deliberation, but also present substantial engineering challenges that must be carefully controlled (API management, prompt optimization, legal move filtering).

\subsection{Aggregating Engines}

Carvalho et al.~\cite{carvalho2017} studied majority voting among homogeneous groups of checkers engines.
They found that group performance improves roughly logarithmically with group size, while outcome variance decreases.

In chess, Spoerer et al.~\cite{spoerer1999} investigated three-member simple majority voting among engines of varying strength.
They analyzed how different configurations (e.g., equal-strength engines versus mixtures of stronger and weaker ones) affect ensemble performance and the variance of game outcomes.
Their results indicate that even very small committees can benefit from majority aggregation, but that gains depend sensitively on the diversity and relative strength of the members, as well as on how ties and disagreements are resolved.
According to this study, to observe a stronger wisdom-of-crowds effect, one should focus on low-Elo engines with small strength differences, which seems to contradict the diversity assumption.
Let us test this.

\section{Methodology and Framework: ChessLab}

%  [Section to be completed in next revision]

\subsection{System Architecture}

ChessLab is a modular, open-source framework designed for large-scale chess engine evaluation. Key components:

\subsubsection{Engine Abstraction Layer}
A common interface (\texttt{BaseEngine}) supports multiple engine types:
\begin{itemize}
  \item \textbf{Stockfish}: UCI protocol with Elo-limiting via \texttt{UCI\_LimitStrength} and \texttt{UCI\_Elo} options.
  \item \textbf{Maia}: Custom interface wrapping the Maia model; configurable skill level (1100--1900 Elo).
  \item \textbf{Random}: Legal move generator for baseline comparisons.
  \item \textbf{LLM}: HuggingFace integration with customizable prompts, legal-move filtering, and sampling strategies.
  \item \textbf{Voting ensembles}: Meta-engine combining other engines via various voting rules.
\end{itemize}

\subsubsection{Game Execution Engine}
Asynchronous game runner that:
\begin{itemize}
  \item Manages chess state (FEN notation, legal moves, draws-by-repetition).
  \item Communicates with engines via UCI protocol or custom APIs.
  \item Records each move, response time, and engine evaluation.
  \item Handles parallel execution (configurable concurrency limit).
  \item Stores complete game records in PostgreSQL.
\end{itemize}

\subsubsection{Persistent Storage}
PostgreSQL schema captures:
\begin{itemize}
  \item \textbf{Players}: Engine type, name, Elo rating, creation timestamp, configuration parameters.
  \item \textbf{Games}: White/Black player IDs, result (1-0, 0-1, 1/2-1/2), PGN, opening, phase metadata.
  \item \textbf{Moves}: Sequence number, SAN/UCI notation, FEN before move, evaluation, move quality (blunder/bad/dubious/ok/good/best).
  \item \textbf{Evaluations}: Depth-annotated Stockfish evaluation for positions; used in post-hoc move-quality analysis.
  \item \textbf{Requests}: Batch queue for asynchronous LLM inference.
\end{itemize}

\subsubsection{Statistical Analysis Module}
Post-game analysis computes:
\begin{itemize}
  \item Elo estimation using the methods described in Section~\ref{subsec:elo}.
  \item Win rate and draw rate across opponent strengths.
  \item Confidence intervals via bootstrap resampling.
  \item Move accuracy (correlation between engine move and best move).
  \item Significance testing (two-sample \(t\)-test, Fisher's exact test for win rates).
\end{itemize}

\subsection{Elo Rating Estimation}
\label{subsec:elo}

To compare ensembles against single engines and to Fouloscopie's human crowd, we estimate Elo ratings using the methods provided:

\subsubsection{Single-Opponent Estimation}
If an ensemble plays \(n\) games against a single opponent of known Elo, and achieves mean score \(s\) (where 1 = win, 0.5 = draw, 0 = loss), the ensemble Elo is:
\begin{equation}
  \text{Elo}_{\text{ensemble}} = \text{Elo}_{\text{opponent}} + 400 \log_{10}\left(\frac{s}{1 - s}\right)
\end{equation}

This formula is derived by inverting Equation~\ref{eqn:expected}, ensuring that the expected score at the estimated Elo matches the observed score.

\subsubsection{Multiple-Opponent Estimation}
If an ensemble plays against multiple opponents at different Elos, we fit a single Elo value that minimizes weighted squared error between observed and expected scores:
\begin{equation}
  \hat{\text{Elo}}_{\text{ensemble}} = \arg\min_{\text{Elo}} \sum_{i=1}^{m} w_i \left( s_i - E(Elo, \text{Elo}_i) \right)^2
\end{equation}
where \(s_i\) is the mean score against opponent \(i\), \(\text{Elo}_i\) is opponent \(i\)'s rating, and \(w_i\) is the weight (typically the number of games against opponent \(i\), normalized). This optimization is solved via bounded scalar minimization (\(250 \leq \text{Elo} \leq 3000\)) with precision 0.1 Elo points.

This approach is robust to mild non-transitivity in the results (e.g., A beats B, B beats C, but C sometimes beats A), and the fitting naturally produces confidence intervals via bootstrap resampling of game outcomes.

\subsection{Experimental Design Principles}

All experiments follow these principles:

\subsubsection{Fair Time Allocation}
Each engine in an ensemble receives equal wall-clock time per move. For Stockfish ensembles, if total time is 5 seconds per move and there are 5 engines, each receives 1 second. This ensures that the ensemble advantage comes from diversity, not resource allocation.

\subsubsection{Deterministic Reproducibility}
All random seeds are fixed and recorded. Stockfish's \texttt{UCI\_Threads} is set to 1 (single thread) to ensure reproducibility across machines. Game variations are introduced through algorithm randomization (Stockfish's evaluation perturbation, Maia's sampling), not external noise.

\subsubsection{Statistical Power}
Experiments use sufficient games to detect meaningful effects (power = 0.8, \(\alpha = 0.05\)). Based on typical chess game outcomes:
\begin{itemize}
  \item Comparing single-engine vs.\ \(N\)-engine ensemble: \(n = 50\) games.
  \item Comparing two ensembles: \(n = 100\) games.
  \item Measuring performance across strength range: \(n = 20\) games per opponent level, \(\approx 200\) games total.
\end{itemize}

\subsubsection{Opponent Strength Variation}
Opponents span at least Elo 1200--2200 (amateur to master strength), with intermediate steps at \(\approx 200\) Elo increments. This enables fitting performance curves and testing predictions from collective intelligence theory.

\section{Experiments}

%  [Section to be completed in next revision]

This section outlines 10 key experiments, prioritized by importance and feasibility. Results will be presented in follow-up publications as experiments complete. For each experiment, we specify:
\begin{itemize}
  \item \textbf{Hypothesis}: The theoretical prediction.
  \item \textbf{Design}: Independent and dependent variables.
  \item \textbf{Metrics}: Primary outcome measures.
  \item \textbf{Analysis}: Planned statistical tests.
\end{itemize}

\subsection{Experiment 1: Homogeneous Stockfish Ensembles (Proof of Concept)}

\subsubsection{Motivation}
Simple baseline establishing that ensemble voting provides measurable advantage over individual engines, even when engines are identical. This tests the most basic prediction: variance reduction through aggregation.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Independent variable}: Ensemble size \(N \in \{1, 3, 5, 10, 20\}\).
  \item \textbf{Engine configuration}: All Stockfish at Elo 1600 (intermediate strength), UCI\_Elo limiting enabled.
  \item \textbf{Opponents}: Stockfish at Elo 1400, 1600, 1800 (opponent weaker, equal, stronger).
  \item \textbf{Games per condition}: 50 games (sufficient to detect win-rate difference of \(\approx 5\%\), \(p < 0.05\)).
\end{itemize}

\subsubsection{Hypothesis}
Ensemble win rate will increase with ensemble size, following a curve of diminishing returns. Mathematically, if individual engines have variance \(\sigma^2\) and uncorrelated errors, the ensemble variance should decrease as \(\sigma^2 / N\). This predicts:
\begin{itemize}
  \item \(N=1\): baseline win rate (e.g., 45\%).
  \item \(N=3\): win rate increase \(\propto \sqrt{3} \approx 1.7\) (error reduction), e.g., 48\% win rate.
  \item \(N \to \infty\): plateau as opponents' skill becomes limiting factor.
\end{itemize}

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate (percentage of wins) and 95\% confidence interval.
  \item Estimated ensemble Elo via multiple-opponent fitting.
  \item Move selection entropy (how often does ensemble choose the modal move vs.\ split votes).
\end{itemize}

\subsubsection{Analysis}
Logistic regression of win rate vs.\ ensemble size. Fitting a power law: \(\text{WinRate}(N) = a + b \log(N)\) or \(\text{WinRate}(N) = c - d / N\). Bootstrap confidence intervals on Elo estimates.

\subsection{Experiment 2: Heterogeneous Elo Ensembles (Testing Page's Diversity Theorem)}

\subsubsection{Motivation}
Test whether diversity in agent capability (not just replication) improves ensemble performance. Page's theorem predicts that cognitive diversity can outweigh individual competence; an ensemble of weak agents with diverse skills should outperform weaker homogeneous groups.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Independent variables}:
        \begin{itemize}
          \item Ensemble composition: \(\{W, M, S\}\) representing weak (1400 Elo), medium (1600 Elo), and strong (1800 Elo) Stockfish.
          \item Ratios tested: 100\% W, 100\% M, 100\% S (homogeneous baselines); 50\% W + 50\% S, 75\% W + 25\% S, 33\% W + 33\% M + 33\% S (heterogeneous).
        \end{itemize}
  \item \textbf{Ensemble size}: Fixed at 5 engines per ensemble.
  \item \textbf{Opponents}: Fixed at Elo 1600 (to test scaling).
  \item \textbf{Games}: 100 games per condition.
\end{itemize}

\subsubsection{Hypothesis}
Ensemble performance follows an inverse-U relationship with diversity:
\begin{itemize}
  \item Pure weak ensemble (100\% 1400): Low performance (e.g., 25\% win rate).
  \item Pure strong ensemble (100\% 1800): High performance (e.g., 75\% win rate).
  \item Optimally diverse ensemble (predicted \(\approx 30\)--40\% weak agents): Superior to homogeneous weak but inferior to homogeneous strong. However, the diversity bonus may make heterogeneous weak ensemble (\(\approx 40\%\) weak, 60\% strong) outperform pure weak or weak-medium blends.
\end{itemize}

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate for each composition.
  \item Estimated ensemble Elo.
  \item Move diversity: entropy of vote distribution (e.g., how often is the vote split vs.\ unanimous).
  \item Error correlation: Do weak and strong engines make uncorrelated mistakes?
\end{itemize}

\subsubsection{Analysis}
Logistic regression with interaction terms for composition. Formal test of Page's theorem: test whether ``diversity bonus'' (predicted performance minus average individual performance) is positive and significant.

\subsection{Experiment 3: Maia Ensemble (Human-Mimetic Diversity)}

\subsubsection{Motivation}
Test whether diversity in \emph{learned} decision-making (Maia) produces similar benefits to diversity in explicit Elo. Maia provides an alternative engine architecture and learned human-like heuristics.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Independent variable}: Maia skill-level composition.
        \begin{itemize}
          \item Homogeneous: 5x Maia-1600, 5x Maia-1400, etc.
          \item Heterogeneous: Mix of Maia-1400, Maia-1600, Maia-1800.
        \end{itemize}
  \item \textbf{Opponents}: Maia at Elo 1400, 1600, 1800 (to ensure fair comparison---playing against own architecture).
  \item \textbf{Games}: 100 per condition.
\end{itemize}

\subsubsection{Hypothesis}
Maia ensembles should exhibit similar diversity effects to Stockfish:
\begin{itemize}
  \item Heterogeneous Maia ensemble outperforms homogeneous Maia ensemble of equivalent average strength.
  \item The diversity bonus magnitude differs from Stockfish (due to different decision-making architecture), but the principle holds.
\end{itemize}

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate and Elo estimates.
  \item Move variety: Do Maia models at different levels vote differently?
  \item Prediction accuracy: Correlation between Maia move choice and human-level move distribution (via Lichess data).
\end{itemize}

\subsubsection{Analysis}
Comparison of heterogeneous vs.\ homogeneous win rates (two-sample \(t\)-test). Estimation of diversity bonus in Elo points.

\subsection{Experiment 4: Voting Strategy Comparison}

\subsubsection{Motivation}
Optimize the aggregation rule. Does weighted voting outperform simple majority? Does surprisingly-popular voting provide theoretical improvement?

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Independent variable}: Voting rule.
        \begin{itemize}
          \item Simple majority (baseline).
          \item Weighted by Elo: Each engine vote counts for \(10^{(\text{Elo} - 1600) / 400}\).
          \item Weighted by evaluation score: Each engine vote counts for \(\max(0, \text{evaluation\_score} / 5)\) (normalized).
          \item Surprisingly popular (LLM/future): Prediction-aware voting.
        \end{itemize}
  \item \textbf{Ensemble}: Fixed heterogeneous Stockfish ensemble (1400, 1600, 1800, repeated).
  \item \textbf{Opponents}: Stockfish 1600, 1800.
  \item \textbf{Games}: 100 per condition.
\end{itemize}

\subsubsection{Hypothesis}
\begin{itemize}
  \item Simple majority: Baseline (\(W_0 = 50\%\)).
  \item Weighted Elo: Moderate improvement (\(W_E \approx 52\%\)) from reducing weak-engine influence.
  \item Weighted evaluation: Small improvement (\(W_V \approx 51\%\)) but risk of overfitting to position score.
  \item Surprisingly popular: Potential improvement (\(W_S \approx 53\%\)) if engines are overconfident or diverse in confidence.
\end{itemize}

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate for each voting rule.
  \item Move agreement: For each position, what fraction of ensemble members voted for the selected move?
  \item Robustness: Does voting rule perform consistently across opponent strengths?
\end{itemize}

\subsubsection{Analysis}
Pairwise comparisons (McNemar's test for win-rate differences). ANOVA for multi-way comparison.

\subsection{Experiment 5: Cross-Architecture Ensemble (Maximum Diversity)}

\subsubsection{Motivation}
Test ensembles combining fundamentally different architectures (search-based, neural learning-based, heuristic). This maximizes cognitive diversity in decision-making approach.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Ensemble composition}: Stockfish-1600 + Maia-1600 + simple-heuristic-1600 (matched Elo when possible).
  \item \textbf{Ensemble variants}:
        \begin{itemize}
          \item 5 engines: 2 Stockfish, 2 Maia, 1 heuristic.
          \item 3 engines: 1 Stockfish, 1 Maia, 1 heuristic (minimal ensemble).
        \end{itemize}
  \item \textbf{Opponents}: Stockfish 1400, 1600, 1800.
  \item \textbf{Games}: 100 per condition.
\end{itemize}

\subsubsection{Hypothesis}
Cross-architecture ensembles achieve higher Elo than same-architecture ensembles of equivalent average strength, due to uncorrelated decision-making errors.

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate and Elo.
  \item Move agreement analysis: How often do different architectures propose identical moves vs.\ different moves?
  \item Error correlation: Compute correlation of move quality rankings between architecture pairs.
\end{itemize}

\subsubsection{Analysis}
Comparison to single-best-engine baseline (Stockfish-1600) via two-sample \(t\)-test. Elo difference estimation.

\subsection{Experiment 6: Fouloscopie Reproduction (Main Contribution)}

\subsubsection{Motivation}
Directly reproduce the human Fouloscopie results using artificial ensembles. This is the keystone experiment: if artificial crowds match human crowd Elo, it suggests collective intelligence principles are domain-independent. If they diverge, it reveals how human and artificial cognition differ.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Crowd composition}: Heterogeneous Stockfish ensemble (sizes: 10, 30, 50 agents), with Elo distribution spanning 1200--2000 (mimicking human skill variation in Fouloscopie).
  \item \textbf{Opponents}: Maia models at Elo 1200, 1400, 1600, 1800, 1900 (spanning weak to very strong).
  \item \textbf{Games}: 20 games per (crowd size, opponent) combination, total \(\approx 300\) games.
  \item \textbf{Voting rule}: Simple majority (matching Fouloscopie).
\end{itemize}

\subsubsection{Hypothesis}
Artificial crowds achieve comparable Elo to human crowds when ensemble size and diversity match. Specifically:
\begin{itemize}
  \item Human Fouloscopie crowd: Estimated Elo \(\approx 2200\) (to be verified from dataset) with win rate \(>60\%\) across all opponent levels.
  \item Artificial crowd (50 Stockfish agents, mixed Elo): Estimated Elo \(\geq 2000\) (90\% of human level) across all opponent strengths.
  \item The ensemble maintains stable performance across opponent range, suggesting robust collective intelligence.
\end{itemize}

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate curve across opponent strength.
  \item Estimated crowd Elo via multi-opponent fitting.
  \item Confidence intervals: Bootstrap 95\% CI on Elo estimate.
  \item Comparison to human Fouloscopie: Relative Elo difference.
\end{itemize}

\subsubsection{Analysis}
Curve fitting: Logistic model of win rate vs.\ opponent Elo. Elo estimation via bounded optimization (described in Section~\ref{subsec:elo}). Hypothesis test: Is artificial crowd Elo statistically indistinguishable from human crowd Elo? (Two-sample \(t\)-test on Elo estimates, with resampling).

\subsection{Experiment 7: Social Influence Analog (Policy Transparency)}

\subsubsection{Motivation}
Fouloscopie found that visible polling (showing vote counts before final voting) amplified collective intelligence. We test an analog: ensemble plays with/without revealing move-frequency counts during the voting phase.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Independent variable}: Information visibility.
        \begin{itemize}
          \item \texttt{Hidden}: Engines vote sequentially; no engine sees others' votes before committing.
          \item \texttt{Visible}: After each engine commits, the cumulative vote count is revealed to remaining engines; they can adjust their strategy.
        \end{itemize}
  \item \textbf{Ensemble}: 5 Stockfish agents at Elo 1600.
  \item \textbf{Opponents}: Stockfish at 1400, 1600, 1800.
  \item \textbf{Games}: 100 per condition.
\end{itemize}

\subsubsection{Hypothesis}
\begin{itemize}
  \item Against weak opponents: Visibility increases win rate (engines align on safe, strong moves).
  \item Against strong opponents: Visibility decreases win rate (engines converge on popular but suboptimal moves, losing diversity).
\end{itemize}

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate by opponent strength.
  \item Vote concentration: Entropy of move distribution (do engines converge or diversify?).
  \item Move quality: Do visible-voting ensembles choose objectively worse moves (lower evaluation) in tough positions?
\end{itemize}

\subsubsection{Analysis}
Two-way ANOVA (visibility \(\times\) opponent strength) on win rate. Interaction plot: Does visibility effect reverse with opponent strength?

\subsection{Experiment 8: Scaling Laws (Crowd Size)}

\subsubsection{Motivation}
Characterize performance as a function of ensemble size. Are there diminishing returns? Can we fit a power law?

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Independent variable}: Ensemble size \(N \in \{1, 3, 5, 10, 20, 50, 100\}\).
  \item \textbf{Ensemble}: Heterogeneous Stockfish (representative mix of Elo 1200--2000).
  \item \textbf{Opponents}: Fixed at Elo 1600 (pivot point).
  \item \textbf{Games}: 50 per size.
\end{itemize}

\subsubsection{Hypothesis}
Performance scales logarithmically or as a power law:
\begin{equation}
  \text{Elo}(N) = \text{Elo}(1) + c \log(N) + \epsilon
\end{equation}
or
\begin{equation}
  \text{Elo}(N) = a - b N^{-\alpha}
\end{equation}
with diminishing returns (slowing improvement at large \(N\)).

\subsubsection{Metrics}
\begin{itemize}
  \item Estimated Elo vs.\ ensemble size.
  \item Goodness-of-fit: \(R^2\) for power law and logarithmic models.
\end{itemize}

\subsubsection{Analysis}
Non-linear least-squares fitting of both models. AIC/BIC model comparison. Extrapolation: At what size do we expect \(\approx 90\%\) of asymptotic performance gain?

\subsection{Experiment 9: LLM Ensemble (Orthogonal Diversity, Future Work)}

\subsubsection{Motivation}
Large Language Models represent a distinct paradigm. Testing them in ensembles addresses questions about whether collective intelligence principles apply beyond traditional game-playing systems.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Engines}: HuggingFace LLM models (Llama-2, Mistral, etc.) with legal-move filtering and sampling.
  \item \textbf{Ensemble}: 5 LLM instances (possibly different models) with shared prompt template.
  \item \textbf{Prompts}: Systematic variation of prompt templates (FEN-only vs.\ move history vs.\ candidate moves).
  \item \textbf{Opponents}: Stockfish at 1400, 1600, 1800.
  \item \textbf{Games}: 50 per condition (LLMs slower, so lower sample size initially).
\end{itemize}

\subsubsection{Hypothesis}
\begin{itemize}
  \item Single LLM: Expected Elo \(\approx 1200\) (untrained/weakly fine-tuned models).
  \item LLM ensemble: Elo \(\approx 1350\) (diversity provides \(\approx 150\) Elo improvement, but still weaker than Stockfish).
  \item Prompt variation: Providing legal-move lists improves ensemble Elo significantly.
\end{itemize}

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate and Elo.
  \item Legal-move compliance: \% of moves that are legal.
  \item Move diversity: Do different LLM instances/prompts suggest different moves?
\end{itemize}

\subsubsection{Analysis}
Comparison to Stockfish baseline. Ablation: Impact of prompt template on Elo.

\subsection{Experiment 10: Time Control Sensitivity (Robustness Check)}

\subsubsection{Motivation}
Verify that results generalize beyond specific time allocations. Elo ratings in chess are calibrated to specific time controls; our experiments must ensure conclusions are robust.

\subsubsection{Design}
\begin{itemize}
  \item \textbf{Independent variable}: Time control.
        \begin{itemize}
          \item Blitz: 3+0 (3 seconds, no increment).
          \item Rapid: 10+0 (10 seconds).
          \item Classical: 30+0 (30 seconds) or depth-limited (10 moves, depth 20).
        \end{itemize}
  \item \textbf{Ensemble}: Fixed 5-engine heterogeneous Stockfish.
  \item \textbf{Opponents}: Stockfish 1600.
  \item \textbf{Games}: 50 per time control.
\end{itemize}

\subsubsection{Hypothesis}
Ensemble advantage is robust across time controls: relative Elo difference between ensemble and single engine remains constant (\(\approx 50\)--100 Elo) regardless of time control.

\subsubsection{Metrics}
\begin{itemize}
  \item Win rate and Elo for each time control.
  \item Ensemble advantage (Elo gain) constancy.
\end{itemize}

\subsubsection{Analysis}
ANOVA for Elo across time controls. Test null hypothesis: Ensemble advantage is independent of time control.

\section{Results}

%  [Section to be completed in next revision]


This section outlines expected findings based on theory and preliminary understanding. These are \textbf{placeholders} to be replaced with actual experimental results.

\subsection{Exp 1: Homogeneous Ensembles}

\textbf{PLACEHOLDER}: We expect win rate to increase from \(\approx 50\%\) (single engine) to \(\approx 52\%\) (\(N=3\)), \(\approx 53\%\) (\(N=5\)), with diminishing returns at larger \(N\). Corresponding Elo gain: \([BASELINE\_ELO] \to [BASELINE\_ELO + 50] \to [BASELINE\_ELO + 80]\), approaching asymptote. **Statistical significance**: Ensemble of 5 will be significantly better than single engine (\(p < 0.05\)).

\subsection{Exp 2: Heterogeneous Elo}

\textbf{PLACEHOLDER}: Optimal diversity at \(\approx 30\)--40\% weak agents. Expected Elo:
\begin{itemize}
  \item 100\% weak (1400): \([Elo\_1400]\) (homogeneous baseline).
  \item 50\% weak + 50\% strong: \([Elo\_MID] \approx \text{Average}\) or slightly above (diversity bonus).
  \item 33\% weak + 33\% mid + 33\% strong: \([Elo\_OPTIMAL]\) (predicted to be highest).
  \item 100\% strong (1800): \([Elo\_1800]\) (homogeneous strong baseline).
\end{itemize}

\subsection{Exp 6: Fouloscopie Reproduction}

\textbf{PLACEHOLDER}: Artificial crowd (50 heterogeneous Stockfish) estimated Elo: [CROWDFISH\_ELO\_EST] (to compare to human Fouloscopie crowd Elo of [HUMAN\_CROWD\_ELO] from dataset). Win rate curve: [CURVE\_DESCRIPTION]. Confidence interval on Elo: \([Elo\_EST] \pm [MARGIN\_ERROR]\).

\section{Discussion and Roadmap}

%  [Section to be completed in next revision]

While this paper focuses on framework and methodology, we can anticipate key insights:

\subsection{Implications for Collective Intelligence Theory}

If experiments confirm that artificial ensembles replicate human Fouloscopie results, it would suggest:
\begin{enumerate}
  \item \textbf{Domain independence}: Collective intelligence principles (diversity, independence, aggregation) apply across human and artificial agents.
  \item \textbf{Mechanism transparency}: Using artificial agents, we can precisely isolate which factors drive collective benefit (e.g., variance reduction vs.\ diversity vs.\ correction of biases).
  \item \textbf{Optimizability}: We can systematically test hypotheses about optimal voting rules, ensemble composition, and information structures that would be intractable with human subjects.
\end{enumerate}

\subsection{Limitations and Future Work}

\subsubsection{LLM Integration}
Currently marked as future work due to engineering complexity (API rate limits, prompt optimization, legal-move filtering). However, LLMs represent the fastest-moving frontier in AI and deserve careful study.

\subsubsection{Opponent Diversity}
Current experiments use same-architecture opponents (e.g., Maia ensemble vs.\ Maia opponents). Future work should include cross-architecture matches (e.g., Stockfish ensemble vs.\ Maia opponents) to test robustness.

\subsubsection{Game Phase Analysis}
Preliminary work should analyze whether ensemble advantage varies by game phase (opening, middlegame, endgame), as opening knowledge and endgame technique may distribute differently across agent types.

\subsubsection{Surprising Popular Moves}
Post-hoc analysis of games should identify moves where the ensemble (or majority) chose a move that was initially surprising (unexpected by others in the ensemble) but turned out to be optimal. This could reveal mechanisms by which diversity prevents groupthink.

\subsubsection{Theoretical Extensions}
Page's Diversity Theorem assumes independence; in ensemble voting, engines share the same position. Future work should explore models of \emph{conditional} independence given the board state, and how architecture diversity affects this.


\section{Conclusion}

%  [Section to be completed in next revision]

This paper introduces ChessLab, a framework for studying collective intelligence in games through systematically engineered ensemble experiments. By replacing human crowds with controlled, diverse AI systems, we gain the ability to:

\begin{enumerate}
  \item Run hundreds of experiments with perfect reproducibility.
  \item Test hypotheses from collective intelligence theory in a complex, sequential domain.
  \item Isolate mechanisms (diversity vs.\ bias correction vs.\ variance reduction) through controlled manipulation.
  \item Compare artificial and human collective intelligence quantitatively (via Elo ratings).
\end{enumerate}

The roadmap of 10 experiments provides a structured progression from simple proof-of-concept (homogeneous ensembles) through sophisticated tests of theory (Page's theorem) and optimization (voting mechanisms) to direct reproduction of human results (Fouloscopie analog).

Results from these experiments will appear in follow-up publications. We anticipate that this work will:

\begin{enumerate}
  \item \textbf{Validate or refine} collective intelligence theory in a new domain.
  \item \textbf{Provide actionable guidance} on ensemble design for game-playing systems.
  \item \textbf{Lay groundwork} for studying collective intelligence in other complex domains (trading, scientific collaboration, etc.).
  \item \textbf{Contribute} to understanding how human and artificial cognition achieve collective goals.
\end{enumerate}

We release ChessLab as open-source software, with the hope that researchers can extend, replicate, and build upon this work.


\section*{Disclaimer}

The assistance of Large Language Models (LLMs) was used in a limited and controlled manner during the writing process of this paper. All scientific claims, experimental designs, theoretical contributions, and the final version of this paper remain the responsibility of the authors.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
