\documentclass[10pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{natbib}
\usepackage[margin=0.75in]{geometry}
\usepackage{abstract}
\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{url}

% Set up abstract formatting
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\small}

% Keywords command
\providecommand{\keywords}[1]
{
  \small
  \textbf{\textit{Keywords---}} #1
}

\lstset{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  commentstyle=\itshape\color{gray},
  keywordstyle=\bfseries\color{blue},
  numberstyle=\tiny\color{gray},
  numbersep=5pt,
  stringstyle=\color{orange},
  frame=single,
  framesep=3pt,
  framerule=0.5pt,
  rulecolor=\color{gray!30},
  backgroundcolor=\color{gray!5}
}

\title{ChessLab: Evaluating the Wisdom of Artificial Crowd in Chess}
\author{Mario Larsen}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  \emph{Wisdom of crowds} is a phenomenon stating that aggregated decisions of diverse, independent agents often outperform individual experts.
  However, crowds of casual chess players still struggle against strong opponents.
  Recent projects tend to show that the method by which a move is chosen by the crowd can significantly influence their performance.
  In this work, we introduce \textbf{ChessLab}, an open-source framework for studying artificial collective intelligence in sequential decision-making through chess.
  By replacing human crowds with heterogeneous ensembles of deterministic and learning-based chess engines, we began investigating questions such as:
  (1) Can voting ensembles of weak agents defeat stronger opponents?
  (2) How does diversity composition affect collective performance?
  (3) What voting mechanisms optimally aggregate engine decisions?
  This paper presents the framework, outlines our experimental methodology, and provides a comprehensive research roadmap for studies to be conducted during the next phase of this project.

  \keywords{chess engines, wisdom of crowds}
\end{abstract}

\section{Introduction}

\emph{Collective intelligence} can in some circumstances exceed individual capability.
One of the earliest demonstrations comes from Galton's famous ox-weighing experiment, where the median guess of approximately 787 individuals in a county fair was more accurate than expert estimators and nearly matched the actual weight of 1,197 pounds~\cite{galton1907}.

This phenomenon has been tested in a variety of settings, such as prediction markets, where Atanasov et al.\ compared prediction markets with prediction polls in a large geopolitical forecasting tournament and found that carefully aggregated team forecasts could outperform market prices~\cite{atanasov2017}.
Question-answering game shows also provide natural experiments.
In ``Who Wants to Be a Millionaire'', the ``Ask the Audience'' lifeline yields correct answers around 87--92\% of the time, whereas phone-a-friend experts succeed only about 55--65\% of the time, across many national editions of the show~\cite{surowiecki2004}.
However, in competitive games like chess, crowds have historically struggled against strong opponents.

The 1999 Kasparov vs.\ The World match saw more than 50,000 human players collectively challenge the World Chess Champion Garry Kasparov~\cite{kasparov1999}, reaching move 62 before conceding.
More recently, in May 2025, over 143,000 players held World Champion Magnus Carlsen to a draw in a record-breaking game on Chess.com~\cite{chess2025}, and in November 2025, over 200,000 players loose in a match against International Master Levy Rozman (GothamChess)~\cite{gothamchess2025}.
However, these matches remain largely anecdotal: a single game provides minimal evidence for systematic collective intelligence, as outcome variance is high and learning effects are confounded.

A Fouloscopie project, led by Mehdi Moussaïd at the Max Planck Institute for Human Development, addressed this limitation by conducting the first rigorous, large-scale experiment on collective chess intelligence~\cite{moussaid2025}.
Approximately 25,000 participants were split into teams to play a total of 500 chess games against AI opponents.
Using majority-vote aggregation of player moves, Moussaïd demonstrated that crowds achieved a performance level similar to a master while being composed predominantly of beginners and casual players.
Importantly, by controlling for poll visibility, the results showed a significant effect of social influence on the crowd's level.

Yet even this landmark study leaves open critical questions:
\begin{itemize}
  \item \textbf{Mechanism understanding}: What aspects of diversity (in skill, size, architecture) drive the ensemble advantage?
  \item \textbf{Optimization}: Can we identify decision-making processes that outperform simple majority aggregation?
  \item \textbf{Domain generality}: Does collective intelligence in chess rely on human-specific cognitive mechanisms, or do the principles apply to artificial agents?
\end{itemize}

We address these questions through \textbf{ChessLab}, an open-source framework for studying artificial collective intelligence in chess.
By substituting human crowds with heterogeneous ensembles of chess engines, we gain precise control over experimental conditions, perfect reproducibility, and the ability to run experiments rapidly with minimal computational cost.

The main contributions of this paper are:

\begin{enumerate}
  \item \textbf{Framework and methodology}: A modular, extensible system for running ensemble chess experiments at scale, with persistent storage of games, moves, evaluations, and statistical analyses.
  \item \textbf{Theoretical grounding}: Clear hypotheses derived from Scott Page's Diversity Prediction Theorem and classical wisdom-of-crowds theory, adapted to the chess domain.
  \item \textbf{Reproducible baselines}: Quantitative comparison against Fouloscopie human results, enabling assessment of whether artificial and human collective intelligence follow similar principles.
  \item \textbf{Comprehensive research roadmap}: A structured plan for experiments addressing collective intelligence across different engine types, ensemble compositions, and voting strategies.
\end{enumerate}

This paper focuses on framework description, experimental design, and hypothesis articulation.
Results from additional experiments will be presented in subsequent publications as they are completed.

\section{Related Work}

\subsection{Collective Intelligence and Wisdom of Crowds}

The wisdom of crowds phenomenon emerged from early empirical observations:
Galton's ox-weighing experiment demonstrated that collective judgments (median of 787 guesses) were more accurate than nearly all individual estimates~\cite{galton1907}.
Surowiecki systematized this observation, arguing that under certain conditions (diversity of opinion, independence of members, decentralization, and effective aggregation) groups reliably outperform experts~\cite{surowiecki2004}.

Formally, Scott Page's \emph{Diversity Prediction Theorem} provides mathematical grounding:
\begin{equation}
  \text{Collective Error} = \overline{\text{Individual Error}} - \text{Diversity}
\end{equation}
where collective error is the mean squared error of the group prediction, \(\overline{\text{Individual Error}}\) is the average of individual squared errors, and Diversity captures how different agents' predictions are~\cite{page2007}.
This result implies that a team of common individuals can outperform a specialist, provided the group is large enough and exhibits sufficient diversity in cognitive approach.

However, diversity alone is insufficient.
Studies show that social influence, information cascades, and herding can destroy the diversity necessary for wisdom of crowds.
Almaatouq et al.\ demonstrated that agents strategically choosing whom to learn from (adaptive networks) better maintain collective intelligence than static networks~\cite{almaatouq2020}.
Koriat and colleagues showed that \emph{confidence} acts as a metacognitive signal:
when individuals can estimate how well-calibrated their judgment is, confidence-weighted aggregation outperforms simple majority voting~\cite{koriat2011}, except when facing common misconceptions that result in misplaced confidence.
Prelec extended this with the ``surprisingly popular'' voting method, which selects answers that are more popular than subjects predicted, revealing hidden consensus even when the majority is wrong~\cite{prelec2017}.

In summary, collective intelligence requires:
(1) a sufficient number of participants,
(2) diversity in decision-making,
(3) independence of judgments,
(4) appropriate aggregation mechanisms, and possibly
(5) calibration of confidence when weighting is available.

\subsection{Collective Intelligence in Chess}

Chess provides an ideal laboratory for studying collective decision-making in complex, sequential tasks.
Unlike static judgment tasks (trivia answering, estimation), chess involves:
\begin{itemize}
  \item \textbf{High-dimensional state space}: Approximately \(10^{47}\) legal positions, making expert heuristics difficult and providing an extensive test dataset.
  \item \textbf{Strategic thinking}: Each player must evaluate not just the next move but plan for responses and counter-responses.
  This task allows deliberation to express emerging strategies.
  \item \textbf{Objective evaluation}: The outcome of a game is unambiguous, and thanks to modern engines, each move can be evaluated with precise numerical scoring.
  The performance level of a crowd can be estimated with arbitrary precision.
  \item \textbf{Cultural significance}: Chess remains a synecdoche for intelligence.
  Both humans and algorithms are expected to demonstrate competence at this task, providing abundant documentation and research literacy.
\end{itemize}


To evaluate the level of a chess player the community mainly rely on a number called the Elo.
Given two players with Elo ratings \(\text{Elo}_A\) and \(\text{Elo}_B\), the expected score (probability of A winning) is:
\begin{equation}
  \label{eqn:expected}
  E_A = \frac{1}{1 + 10^{(\text{Elo}_B - \text{Elo}_A) / 400}}
\end{equation}
This is the standard Elo formula, calibrated for chess~\cite{elo1978}.

While this metric captures relative strength between two players, its absolute interpretation depends heavily on methodology and context.
Community conventions suggest that players understanding the basic rules rate between 600 and 900 Elo, casual players typically orbit around 1300 Elo, and from 2000 Elo onward, players enter the master and grandmaster territory.
At his peak in May 2014, Magnus Carlsen achieved a classical rating of 2882, the highest in history.
He also achieved 2909 in the newly-introduced Freestyle Chess (Chess960) rating system~\cite{carlsen2025}.

\subsection{Empirical Studies of Crowd Chess}

The Kasparov vs.\ The World match (1999) is the earliest documented crowdsourced chess challenge.
Over four months, approximately 50,000 amateur and intermediate players voted on moves against the then-World Champion.
Despite reaching an advanced position, the crowd eventually conceded after 62 moves when facing maximum resistance from Kasparov~\cite{kasparov1999}.
While historically significant, this single-game format provides weak evidence: a single outcome cannot be reliably attributed to collective intelligence versus chance.

The Fouloscopie experiment rectified this limitation through large-scale repetition~\cite{moussaid2025}.
Moussaïd and colleagues organized approximately 25,000 human participants to play a total of 500 games against chess engine opponents.
Key design features included:
\begin{itemize}
  \item \textbf{Diverse player population}: Participants self-reported their Elo rating; the population was approximately normally distributed with a mean around 1165 Elo.
  \item \textbf{Majority-vote aggregation}: Players had several hours to evaluate positions and vote on the next move; the most-voted move was played automatically.
  \item \textbf{Opponent variety}: Chess engine opponents (Maia models) ranged from weak (1100 Elo) to strong (1900 Elo), allowing measurement of performance across the strength spectrum.
  \item \textbf{Social influence manipulation}: Comparing voting-only versus poll-visibility conditions revealed how information visibility affects collective decision quality.
\end{itemize}

Results showed that crowds achieved a win rate exceeding 60\% across all tested opponents.
With poll visibility, win rates improved to approximately 77\%, while visibility-hidden conditions showed approximately 64\% win rates.
Importantly, the crowd played measurably better than the average individual member, confirming classic wisdom-of-crowds predictions.

\subsection{Chess Engines}

Modern chess engines span multiple paradigms, each with distinct decision-making processes.
We focus on three particularly relevant approaches:

\subsubsection{Stockfish}

Stockfish, as of 2025, is the world's strongest chess engine.
It is an open-source tree-search algorithm combining classical techniques (alpha-beta pruning, endgame tables) with small neural networks for position evaluation~\cite{stockfish2024}.
Its strength is calibrated via Elo ratings: users can set \texttt{UCI\_Elo} to any value from 1320 to 3190.
The engine deliberately weakens its play through stochastic evaluation perturbation (seeded random noise), ensuring exploration of different move trees at reduced strength levels.

\subsubsection{Maia: Human-Mimetic Neural Networks}

In contrast to optimal play, Maia learns to \emph{predict human moves} from millions of Lichess games~\cite{mcilroy-young2020}.
Rather than computing optimal moves, Maia generates a probability distribution over legal moves, parameterized by player skill level.
Maia-2, introduced at NeurIPS 2024, incorporates a skill-aware attention mechanism that dynamically integrates player skill level with board position encoding~\cite{maia2024}.
This mechanism achieves approximately 50--52\% top-1 move accuracy across human players rated 1100 to 1900 Elo.
In practical play, Maia replicates human decision-making nearly indistinguishably, making it an excellent tool for evaluating crowd performance against human-like opponents.

\subsubsection{Large Language Models}

Recent work demonstrates that Large Language Models (LLMs) trained on chess game transcriptions can develop emergent internal representations of board states~\cite{karvonen2024}.
These representations can be altered, thereby changing the move predicted by the LLM.
This represents a fundamentally different paradigm: LLMs make decisions based on statistical pattern recognition from training data rather than explicit search or neural evaluation.
Their move quality is unstable and depends on a myriad of factors (model architecture, PGN-based prompting, board representation, move history), which are often difficult to replicate or interpret.% TODO add citation for https://arxiv.org/abs/2512.01992

Carlini showed that \texttt{gpt-3.5-turbo-instruct} can achieve a playing strength of roughly 1,788 Elo under specific conditions~\cite{carlini2023}.
More recent transformer-based models have achieved grandmaster-level performance when trained directly on game transcriptions~\cite{schrittwieser2024}.

Although modern chat-oriented LLMs appear to lag behind, Dynomight~\cite{dynomight2024} demonstrated that prompting strategies—such as requiring the model to regenerate the move history before outputting the next move—can dramatically improve their move quality.
These results suggest that apparent “unusual” chess behavior in some LLMs may stem less from intrinsic chess competence and more from specific training setups and prompt configurations.

For ensemble experiments, LLMs offer the greatest cognitive diversity and potential for open deliberation, but also present substantial engineering challenges that must be carefully controlled (API management, prompt optimization, legal move filtering).

\subsection{Aggregating Engines}

Carvalho et al.~\cite{carvalho2017} studied majority voting among homogeneous groups of checkers engines.
They found that group performance improves roughly logarithmically with group size, while outcome variance decreases.

In chess, Spoerer et al.~\cite{spoerer1999} investigated three-member simple majority voting among engines of varying strength.
They analyzed how different configurations (e.g., equal-strength engines versus mixtures of stronger and weaker ones) affect ensemble performance and the variance of game outcomes.
Their results indicate that even very small committees can benefit from majority aggregation, but that gains depend sensitively on the diversity and relative strength of the members, as well as on how ties and disagreements are resolved.
According to this study, to observe a stronger wisdom-of-crowds effect, one should focus on low-Elo engines with small strength differences, which seems to contradict the diversity assumption.
Let us test this.

\section{Methodology and Framework}

ChessLab is a modular, open-source framework for large-scale chess engine evaluation and crowd experimentation. 
In this section we will details some of its inner working and base modules.

\subsection{UCI Engines}

Classically, chess engines communicate with an interface named Universal Chess Interface (UCI).% TODO ADD CITATION like toward the wiki of computational chess or lichess or any paper that details this interface
We leverages this protocol to integrate any UCI-compliant engine, such as Stockfish. 
For engines or models that do not expose UCI (for example, Maia), we provide a custom wrapper interface.
In addition, we implement several synthetic agents internally (e.g., random baselines and LLM-based agents).
By exposing all engines through a common abstraction, we can treat them uniformly and combine them into a “crowd” engine governed by a configurable aggregation rule.

\subsection{Aggregators Rules}

We define a family of aggregation rules that map a set of engine proposals into a single move choice.
Examples include:

\begin{itemize}
  \item \textbf{Majority}: Each engine proposes a single move; the move with the highest vote count is played, with ties broken randomly.
  \item \textbf{Minority}: The least popular move among the proposed ones is selected, intended as an extreme contrarian baseline.
  \item \textbf{Randomized}: One of the proposed moves is sampled at random, with or without weighting by engine Elo.
  \item \textbf{Top-Elo dictator}: The move proposed by the highest-rated engine in the ensemble is always selected.
  \item \textbf{Bottom-Elo dictator}: Symmetric to the top-Elo dictator, using the lowest-rated engine; serves as a control for “anti-expert” aggregation.
  \item \textbf{Median-Elo dictator}: The move from the median-rated engine is chosen, approximating a representative member of the group.
  \item \textbf{Rotating dictator}: Engines take turns acting as dictator across moves or games, ensuring equal participation over time.
  \item \textbf{Elo-weighted}: Each engine casts a vote weighted by a function of its Elo (e.g., proportional to Elo, or logistic in Elo); the move with the highest total weight is selected.
\end{itemize}

These rules are not mend to lead optimal result but to compare against each other and against there use in human crowd.

%TODO: Maybe introduce a unifying formalism (e.g., defining each rule as an aggregation function with math symbols) and relate specific rules to diversity/independence assumptions from COMSOC theory. (cite the comsoc introduction paper by ulle endriss)

\subsection{Game Execution Engine}

The core of ChessLab is a Python-based game runner built on top of the \texttt{python-chess} library, which manages internal move generation and game-state transitions.
Games are executed asynchronously using \texttt{asyncio}, allowing highly parallelized evaluation across many engine-versus-engine matchups.

The PostgreSQL schema is organized around the following core entities:

\begin{itemize}
  \item \textbf{Players}: Engine type (e.g., Stockfish, Maia, LLM, random), name, nominal Elo rating, creation timestamp, and configuration parameters such as depth limits or temperature settings.
  \item \textbf{Games}: References to white and black player IDs, the numeric result (1--0, 0--1, 1/2--1/2), full PGN, and metadata about the game.
  \item \textbf{Moves}: For each game, a sequence of moves with SAN and UCI representations, FEN preceding the move.
  \item \textbf{Evaluations}: For a given move, the required engines (player engine or multiple engines in case of a crowd) make an evaluation (in centipawns or similar score).
\end{itemize}

\subsection{Statistical Analysis Module} % TODO

% After games are completed, a post-game analysis pipeline computes a suite of performance metrics for each engine or ensemble.
% This module is implemented as a separate component that queries the database, runs analyses, and stores derived statistics.

% Specifically, the analysis module computes:

% \begin{itemize}
%   \item Elo estimation using the procedures described in Section~\ref{subsec:elo}.
%   \item Win rate and draw rate as a function of opponent strength, aggregated across multiple opponent Elo bins.
%   \item Bootstrapped confidence intervals on win-rate and Elo estimates, providing uncertainty bands for cross-engine comparisons.\footnote{\url{https://www.sciencedirect.com/science/article/abs/pii/S0169207020300157}}
%   \item Move accuracy, defined via similarity between each engine’s chosen move and the move a strong reference engine rates as best, along with label-based error counts (e.g., blunders, inaccuracies).\footnote{\url{https://www.sciencedirect.com/science/article/abs/pii/S0950584925000187}}
%   \item Significance tests such as two-sample \(t\)-tests on continuous scores and Fisher’s exact test on win-rate proportions, to assess whether performance differences between engines or ensembles are statistically meaningful.
% \end{itemize}

% %TODO: Specify exact statistical packages used, multiple-comparison corrections (if any), and criteria for practical vs.
% statistical significance.

% \subsection{Elo Rating Estimation}\label{subsec:elo}

% To compare ensembles against single engines and against human-crowd Elo estimates (when available, e.g., from Fouloscopie-style experiments), we derive Elo ratings using standard adjustment rules adapted to our experimental setup.\footnote{\url{https://en.wikipedia.org/wiki/Elo_rating_system}}

% \subsubsection{Single-Opponent Estimation}

% When an ensemble plays \(n\) games against a single opponent of known Elo, let \(s\) be the ensemble’s mean score per game (1 for a win, 0.5 for a draw, 0 for a loss).
% Using the standard logistic pairwise-comparison law, the ensemble’s Elo is estimated as

% \begin{equation}
% \text{Elo}_{\text{ensemble}} = \text{Elo}_{\text{opponent}} + 400 \log_{10}\left(\frac{s}{1 - s}\right).
% \end{equation}

% This formula inverts the expectation relation~\eqref{eqn:expected}, so that the expected score given the estimated Elo matches the observed score \(s\).

% %TODO: Clarify how cases with $s = 0$ or $s = 1$ are handled (e.g., score clipping or Bayesian priors).

% \subsubsection{Multiple-Opponent Estimation}

% In practice, evaluating an engine or ensemble typically requires a diverse set of opponents, so we calibrate its Elo against a ladder of reference engines spanning varying strengths.\footnote{\url{https://www.sciencedirect.com/science/article/abs/pii/S0169207020300157}}

% We consider:

% \begin{itemize}
%   \item A collection of Stockfish instances configured to cover roughly 1300--3500 Elo (with primary focus on the 1300--2100 band), using UCI strength-limiting options.\footnote{\url{https://arxiv.org/pdf/2007.02130.pdf}}
%   \item A ladder of Maia-style models ranging from about 1100 to 1900 Elo, when available.\footnote{\url{https://arxiv.org/pdf/2405.05066.pdf}}
%   \item For very weak engines, a supplementary set of low-strength opponents or handicapped engines to avoid ceiling effects and saturation, following calibration practices similar to “Elo World”-style procedures.\footnote{\url{https://www.sciencedirect.com/science/article/abs/pii/S0169207020300157}}
% \end{itemize}

% When the ensemble plays against \(m\) distinct opponents at known Elos \(\text{Elo}_i\) with mean score \(s_i\) against opponent \(i\), we fit a single Elo value that minimizes a weighted squared-error loss between observed and expected scores:

% \begin{equation}
% \hat{\text{Elo}}_{\text{ensemble}} = \arg\min_{\text{Elo}} \sum_{i=1}^{m} w_i \left( s_i - E(\text{Elo}, \text{Elo}_i) \right)^2,
% \end{equation}

% where \(E(\text{Elo}, \text{Elo}_i)\) is the expected score according to the logistic pairwise model, and each weight \(w_i\) is proportional to the number of games against opponent \(i\) (normalized so that \(\sum_i w_i = 1\)).
% Optimization is performed via bounded scalar minimization over \(\text{Elo} \in [250, 3000]\) with a precision of 0.1 Elo points.

% This approach is robust to mild violations of transitivity (for instance, \(A\) beats \(B\), \(B\) beats \(C\), but \(C\) occasionally beats \(A\)), because the least-squares fit averages conflicting outcomes across different matchups.
% To quantify uncertainty, we pair the fitting procedure with bootstrap resampling over outcome sequences, which yields confidence intervals on \(\hat{\text{Elo}}_{\text{ensemble}}\).\footnote{\url{https://en.wikipedia.org/wiki/Elo_rating_system}}

% %TODO: Report typical confidence interval widths for key experiments and discuss how many games are required to achieve a given Elo precision.


\section{Proof-of-Concept Experiments}

In this section we present preliminary experiments designed to validate ChessLab’s calibration procedures and demonstrate basic ensemble behaviors.
These results serve as sanity checks and motivate the more extensive roadmap outlined in Section~\ref{sec:roadmap}.

To permit fast execution, we limit initial experiments to ensembles of up to 5 engines playing 20 games each against a small ladder of Stockfish opponents (ELO ranging from 1300 to 2100).
% TODO verify those values in the files

\subsection{Calibration}

We first evaluate both Stockfish and Maia against a ladder of Stockfish opponents to verify that our Elo estimation pipeline recovers their intended strength profiles.
Across the tested range, the estimated Elo values track the nominal calibration suggested by the engine authors, albeit with systematic deviations at the extremes of the scale.

%TODO: Insert calibration plots (estimated vs.\ target Elo) and quantify deviations (e.g., mean absolute error in Elo points).

\subsection{Stockfish Crowd}

We then construct ensembles composed solely of low- to mid-strength Stockfish instances and compare several aggregation rules.
Preliminary results indicate that majority voting yields performance in a similar Elo range to the top-Elo dictator, suggesting limited marginal gains from simple voting among homogeneous engines at these settings.

%TODO: Provide detailed win/draw/loss statistics, estimated Elo for each aggregation rule, and at least one illustrative game where the ensemble corrects a single-engine mistake.

\subsection{Local LLM}

Finally, we test several local LLMs with naive prompting (board description plus “what is the best move?”) against the weakest Stockfish configurations.
In this setting, the LLM agents fail to win a single game and often commit basic tactical errors, confirming that unoptimized LLM chess prompting can yield extremely low effective Elo.

These results highlight both the potential and the challenges of incorporating LLM-based agents into ChessLab: substantial prompt engineering, legality checks, and perhaps fine-tuning are required before LLMs can serve as competitive and diverse members of an ensemble.

%TODO: Specify which local models were used, how board states were encoded in the prompt, and include examples of typical LLM failure modes.

\section{Roadmap}\label{sec:roadmap}

The current work establishes ChessLab’s core infrastructure and demonstrates basic feasibility, but many key questions about artificial collective intelligence in chess remain open.
We outline here a roadmap of planned experiments and framework extensions.

Planned experiments include:

\begin{itemize}
  \item Playing against a range of Maia opponents instead of Stockfish, to better align with human-like play.
  \item Increasing the number of runs and the size of the crowds testes thanks to better computers
  \item Calibrating distilled Stockfish and Maia variants to obtain lower elo opponents. A distilled engine is an engine having a specified probability to play randomly% TODO ADD CITATION to sogbivik tom 7  chess article: elo world
  \item Reproducing the Moussaïd’s experiment as closely as possible, using estimates of participant strength, and comparing human and artificial crowds under analogous conditions.
  \item Systematically exploring LLM prompting strategies (e.g., reconstructing move history, chain-of-thought reasoning) to improve LLM move quality and stability. % CITE dynomight2024
  \item For sufficiently capable LLMs, attempting to assign effective Elo ratings and integrating them into mixed-architecture ensembles.
  \item Having LLMs output their top-\(k\) candidate moves with rationales, enabling more sophisticated aggregation rules (e.g., Condorcet, Borda, Coombs, Approval voting).
  \item Studying information-sharing regimes, such as letting LLMs observe current poll results before voting, to simulate social influence dynamics.
  \item Investigating argumentation pipelines in which multiple LLMs propose moves and arguments, and a separate ``arbiter'' LLM selects the final move.
\end{itemize}

Planned framework extensions include:

\begin{itemize}
  \item Adding tactical puzzle modes to estimate Elo via centipawn loss on curated positions. Accelerating preliminary testing on LLM based engines.
  \item Integrating external API-based LLMs, with rate-limit-aware scheduling in the request queue.
  \item Providing containerized deployments (e.g., Docker images) for reproducible local and cloud experiments.
  \item Developing a web interface to share experiments with a broader audience and facilitate crowdsourced research contributions.
\end{itemize}

\section{Conclusion}

This paper introduces ChessLab, a framework for studying collective intelligence in chess by replacing human crowds with ensembles of artificial agents.
The system enables hundreds of reproducible, low-cost experiments that test hypotheses from wisdom-of-crowds theory in a complex, sequential decision-making domain.
By instrumenting games with detailed move-level evaluations and robust Elo estimation, ChessLab makes it possible to disentangle ensemble benefits arising from diversity, bias correction, and variance reduction, and to compare artificial and human crowds on a common quantitative scale.

The roadmap from baseline replication of human experiments to optimization of voting mechanisms and information structures is designed to progressively bridge empirical chess results and broader theories of collective intelligence.

We release ChessLab as open-source software and invite the community to extend, replicate, and build upon this work: \url{https://github.com/Uspectacle/chesslab}
Future publications will report on the larger set of experiments outlined here and on the resulting refinements to our understanding of artificial and human crowd wisdom in strategic games.

\section*{Disclaimer}

The assistance of Large Language Models (LLMs) was used in a limited and controlled manner during the writing process of this paper.
All scientific claims, experimental designs, theoretical contributions, and the final version of this paper remain the responsibility of the authors.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
